{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d535c085",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Surfer Detection - Training and Evaluation Pipeline\n",
    "# This script trains YOLOv8 on a folder of labeled surfer images and evaluates performance\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "from datetime import datetime\n",
    "\n",
    "class SurferDetectionTrainer:\n",
    "    def __init__(self, \n",
    "                 data_dir,\n",
    "                 output_dir=\"surfer_detection_results\",\n",
    "                 train_val_split=0.8,\n",
    "                 test_size=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the surfer detection training pipeline\n",
    "        \n",
    "        Args:\n",
    "            data_dir: Directory containing images and labels in YOLO format\n",
    "            output_dir: Directory to save results\n",
    "            train_val_split: Ratio of training to validation data\n",
    "            test_size: Portion of data to use for testing\n",
    "        \"\"\"\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.train_val_split = train_val_split\n",
    "        self.test_size = test_size\n",
    "        self.model_types = [\"yolov8n\", \"yolov8s\", \"yolov8m\"]  # Models to evaluate\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize directories for dataset splits\n",
    "        self.dataset_dir = self.output_dir / \"dataset\"\n",
    "        \n",
    "        # Results storage\n",
    "        self.results = {}\n",
    "        \n",
    "    def prepare_dataset(self):\n",
    "        \"\"\"Prepare the dataset by organizing files and creating data.yaml\"\"\"\n",
    "        print(\"Preparing dataset...\")\n",
    "        \n",
    "        # Create dataset directory structure\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            os.makedirs(self.dataset_dir / \"images\" / split, exist_ok=True)\n",
    "            os.makedirs(self.dataset_dir / \"labels\" / split, exist_ok=True)\n",
    "        \n",
    "        # Get all image files\n",
    "        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp']\n",
    "        image_files = []\n",
    "        for ext in image_extensions:\n",
    "            image_files.extend(list(self.data_dir.glob(f\"*{ext}\")))\n",
    "        \n",
    "        # Shuffle files for random split\n",
    "        random.shuffle(image_files)\n",
    "        \n",
    "        # Calculate split sizes\n",
    "        total_files = len(image_files)\n",
    "        test_count = int(total_files * self.test_size)\n",
    "        train_count = int((total_files - test_count) * self.train_val_split)\n",
    "        val_count = total_files - test_count - train_count\n",
    "        \n",
    "        print(f\"Found {total_files} images. Split: {train_count} train, {val_count} validation, {test_count} test\")\n",
    "        \n",
    "        # Function to copy files for a given split\n",
    "        def copy_files_for_split(files, split):\n",
    "            for img_path in files:\n",
    "                # Get corresponding label file\n",
    "                label_path = self.data_dir / f\"{img_path.stem}.txt\"\n",
    "                \n",
    "                # Skip if label doesn't exist\n",
    "                if not label_path.exists():\n",
    "                    print(f\"Warning: No label found for {img_path.name}, skipping\")\n",
    "                    continue\n",
    "                \n",
    "                # Copy image and label to respective directories\n",
    "                shutil.copy(img_path, self.dataset_dir / \"images\" / split / img_path.name)\n",
    "                shutil.copy(label_path, self.dataset_dir / \"labels\" / split / label_path.name)\n",
    "        \n",
    "        # Split and copy files\n",
    "        copy_files_for_split(image_files[:train_count], \"train\")\n",
    "        copy_files_for_split(image_files[train_count:train_count+val_count], \"val\")\n",
    "        copy_files_for_split(image_files[train_count+val_count:], \"test\")\n",
    "        \n",
    "        # Create data.yaml file\n",
    "        data_yaml = {\n",
    "            'path': str(self.dataset_dir.absolute()),\n",
    "            'train': str(self.dataset_dir / \"images\" / \"train\"),\n",
    "            'val': str(self.dataset_dir / \"images\" / \"val\"),\n",
    "            'test': str(self.dataset_dir / \"images\" / \"test\"),\n",
    "            'nc': 1,  # Number of classes\n",
    "            'names': ['surfer']  # Class names\n",
    "        }\n",
    "        \n",
    "        with open(self.dataset_dir / \"data.yaml\", 'w') as f:\n",
    "            yaml.dump(data_yaml, f)\n",
    "        \n",
    "        print(f\"Dataset prepared at {self.dataset_dir}\")\n",
    "        return self.dataset_dir / \"data.yaml\"\n",
    "    \n",
    "    def train_and_evaluate_models(self):\n",
    "        \"\"\"Train and evaluate different YOLO models\"\"\"\n",
    "        print(\"Starting model training and evaluation...\")\n",
    "        data_yaml_path = self.prepare_dataset()\n",
    "        \n",
    "        for model_name in self.model_types:\n",
    "            print(f\"\\n{'-'*50}\")\n",
    "            print(f\"Training {model_name}...\")\n",
    "            print(f\"{'-'*50}\")\n",
    "            \n",
    "            # Create model directory\n",
    "            model_dir = self.output_dir / model_name\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "            \n",
    "            # Initialize model\n",
    "            model = YOLO(f\"{model_name}.pt\")\n",
    "            \n",
    "            # Train model\n",
    "            results = model.train(\n",
    "                data=str(data_yaml_path),\n",
    "                epochs=50,  # Reduced for MVP, increase for production\n",
    "                patience=10,\n",
    "                batch=16,\n",
    "                imgsz=640,\n",
    "                save=True,\n",
    "                project=str(model_dir),\n",
    "                name=\"train\",\n",
    "                device='0' if self.check_gpu_available() else 'cpu',\n",
    "                verbose=True\n",
    "            )\n",
    "            \n",
    "            # Validate on test set\n",
    "            print(f\"Validating {model_name} on test set...\")\n",
    "            val_results = model.val(\n",
    "                data=str(data_yaml_path),\n",
    "                split='test',\n",
    "                project=str(model_dir),\n",
    "                name=\"val\"\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            self.results[model_name] = {\n",
    "                'metrics': val_results.box,\n",
    "                'model_path': model_dir / \"train\" / \"weights\" / \"best.pt\"\n",
    "            }\n",
    "            \n",
    "            print(f\"{model_name} training and validation complete\")\n",
    "        \n",
    "        # Find the best model\n",
    "        best_model = self.determine_best_model()\n",
    "        return best_model\n",
    "    \n",
    "    def determine_best_model(self):\n",
    "        \"\"\"Determine the best model based on evaluation metrics\"\"\"\n",
    "        print(\"\\nEvaluating model performance...\")\n",
    "        \n",
    "        # Compare metrics (primarily using mAP50-95)\n",
    "        metrics_data = {}\n",
    "        for model_name, data in self.results.items():\n",
    "            metrics = data['metrics']\n",
    "            metrics_data[model_name] = {\n",
    "                'mAP50-95': metrics.map,\n",
    "                'mAP50': metrics.map50,\n",
    "                'precision': metrics.p,\n",
    "                'recall': metrics.r,\n",
    "                'model_path': data['model_path']\n",
    "            }\n",
    "            print(f\"{model_name}: mAP50-95={metrics.map:.4f}, mAP50={metrics.map50:.4f}, precision={metrics.p:.4f}, recall={metrics.r:.4f}\")\n",
    "        \n",
    "        # Find best model based on mAP50-95\n",
    "        best_model = max(metrics_data.items(), key=lambda x: x[1]['mAP50-95'])\n",
    "        best_model_name, best_metrics = best_model\n",
    "        \n",
    "        print(f\"\\nâœ… Best model: {best_model_name} with mAP50-95={best_metrics['mAP50-95']:.4f}\")\n",
    "        \n",
    "        # Copy best model to root of output directory\n",
    "        best_model_path = best_metrics['model_path']\n",
    "        best_model_copy_path = self.output_dir / \"best_model.pt\"\n",
    "        shutil.copy(best_model_path, best_model_copy_path)\n",
    "        print(f\"Best model saved to {best_model_copy_path}\")\n",
    "        \n",
    "        # Create performance comparison plot\n",
    "        self.plot_model_comparison(metrics_data)\n",
    "        \n",
    "        return {\n",
    "            'model_name': best_model_name,\n",
    "            'metrics': best_metrics,\n",
    "            'model_path': str(best_model_copy_path)\n",
    "        }\n",
    "    \n",
    "    def plot_model_comparison(self, metrics_data):\n",
    "        \"\"\"Create a plot comparing model performance\"\"\"\n",
    "        models = list(metrics_data.keys())\n",
    "        map_values = [metrics_data[m]['mAP50-95'] for m in models]\n",
    "        precision_values = [metrics_data[m]['precision'] for m in models]\n",
    "        recall_values = [metrics_data[m]['recall'] for m in models]\n",
    "        \n",
    "        x = range(len(models))\n",
    "        width = 0.25\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.bar([i - width for i in x], map_values, width, label='mAP50-95')\n",
    "        ax.bar(x, precision_values, width, label='Precision')\n",
    "        ax.bar([i + width for i in x], recall_values, width, label='Recall')\n",
    "        \n",
    "        ax.set_ylabel('Scores')\n",
    "        ax.set_title('Model Performance Comparison')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(models)\n",
    "        ax.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir / \"model_comparison.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    def check_gpu_available(self):\n",
    "        \"\"\"Check if a GPU is available for training\"\"\"\n",
    "        try:\n",
    "            import torch\n",
    "            return torch.cuda.is_available()\n",
    "        except ImportError:\n",
    "            return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the training pipeline\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"SURFER DETECTION - MODEL TRAINING PIPELINE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get input directory from user\n",
    "    data_dir = input(\"Enter the path to your dataset directory (containing images and labels): \")\n",
    "    \n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"Error: Directory {data_dir} does not exist!\")\n",
    "        return\n",
    "    \n",
    "    # Initialize trainer\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = f\"surfer_detection_results_{timestamp}\"\n",
    "    \n",
    "    trainer = SurferDetectionTrainer(\n",
    "        data_dir=data_dir,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "    \n",
    "    # Run training pipeline\n",
    "    print(\"\\nStarting training pipeline...\")\n",
    "    best_model = trainer.train_and_evaluate_models()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"TRAINING PIPELINE COMPLETE\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Best model: {best_model['model_name']}\")\n",
    "    print(f\"mAP50-95: {best_model['metrics']['mAP50-95']:.4f}\")\n",
    "    print(f\"Model saved to: {best_model['model_path']}\")\n",
    "    print(f\"All results saved to: {output_dir}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
